# TBDM
Repository for the "Technologies for Big Data Management" course, held by professor Massimo Callisto De Donato - University of Camerino, Italy.

# Participants

Damiano Buzzo - damiano.buzzo@studenti.unicam.it
Francesco Pizzuto - francesco.pizzuto@studenti.unicam.it
Eduardo Tiano - eduardovincenz.tiano@studenti.unicam.it

# Scope of the project

The project focuses on the analysis of data generated by sensors connected to a dewatering machine, considered a CPS (Cyber Physical System), utilising Elasticsearch, Kibana, and Kafka; they're, as the name of the course suggests, _technologies for Big Data management_.

The project involves many features. First of all, a streaming pipeline has been created, using MQTT to simulate the real-time streaming of data into Kafka. Then, Kafka has been connected to Elasticsearch, which uses Kibana for visualisation. Inside Kibana, some dashboards with relevant data have been produced and are available for interaction. A map visualisation has been provided too. The project also integrates a .NET connector, for communication with a custom endpoint written in C#. The project also investigated the integrated ML model, useful to obtain insights about historical data. Finally, a real-time watcher has been configured to log critical parameters for real-time data.

# Description and prerequisites

The project has been developed using Docker, a development tool that permits a fast, platform-independent deployment, running applications in containers. Each container represents and independent unit that is isolated from the others, but collaborates to realise the application's behaviour. The user needs to install Docker(https://www.docker.com/products/docker-desktop/), and have a stable Python version(https://www.python.org/downloads/) before proceeding in the guide.
It is also necessary to execute some REST APIs. Postman tool(https://www.postman.com/downloads/) is recommended.

Optionally, the user can install MQTT Explorer(http://mqtt-explorer.com/) to visualise MQTT messages.

It's highly recommended to allocate at least 16 gigabytes of RAM memory for Docker, to avoid performance issues, and a reasonably high number of processors (not all the available ones on the host machine). Edits can be performed modifying the wsl.config(https://learn.microsoft.com/en-gb/windows/wsl/wsl-config) file.

# Installation

To run the project, the user must clone this repository in a directory using the Git command:
```
git clone https://github.com/edunando15/TBDM.git
```
and then open Docker. Then, the user shall type in the same directory in which the repository has been cloned:
```
docker-compose up -d
```
to startup the initialisation process, which will be automatically performed by Docker, installing all the necessary files.

# Configuration

Once the process has ended and all containers are running, the user shall import the file called "TBDM.postman_collection.json" into Postman, and execute the "MQTT Connector Configuration" and "Elasticsearch Connector Configuration" APIs. This creates 2 connectors: one between MQTT and Kafka, and another between Kafka and Elasticsearch. To check that both run seamlessly, the user can execute the "Get Active Connectors" API. The two connectors should be shown.

# Ready-to-use guide

Users reaching this point are able to stream any data in JSON format to an MQTT topic called "new-test-topic". Data will be received by Elasticsearch.

To inject the machine data into Elasticsearch, the user can either open the Elasticsearch UI, clicking on the port section of the container named "kibana" in Docker, and then, in the discover section, upload the CSV file manually (the file is located in MQTTStreamer/Resources/dati_macchina_copy.csv); or execute the FullUploader.py file in the MQTTStreamer directory. To execute the script, it can either be opened in and IDE and executed, or the user can:
- open the terminal;
- type ```cd path/to/MQTTStreamer```;
- type ```python FullUploader.py```

At this point, data should be visible both in Kibana and Kafka, and MQTT Explorer as well.


# Dashboards, map, alert
